

\documentclass[twofold]{article}

%\usepackage[sc]{mathpazo}
\usepackage{lmodern}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\linespread{1.05}
\usepackage{microtype}


\usepackage[english]{babel}

\usepackage[hmarginratio=1:1, top=32mm,columnsep=20pt]{geometry}
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption}
\usepackage{booktabs}

\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{abstract}
\renewcommand{\abstractnamefont}{\normalfont\bfseries}
\renewcommand{\abstracttextfont}{\normalfont\small}

\usepackage{lettrine}

%\usepackage{titlesec}
%\renewcommand\thesection{\Roman{section}}
%\renewcommand\thesubsection{\roman{subsection}}
%\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{}
%\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[C]{Pranshu Gaba $\bullet$ Summer Project 2017 $\bullet$ Sr. No. 13718}
\fancyfoot[RO, LE]{\thepage}


\usepackage{titling}
\usepackage{hyperref}

\usepackage{mathtools}

%\setlength{\droptitle}{-6\baselineskip}
\pretitle{\begin{center}\huge\bfseries}
\posttitle{\end{center}}


\newcommand*\conj[1]{\overline{#1}}
\newcommand*\adj[1]{#1^*}
\newcommand*\norm[1]{\left \Vert #1 \right\Vert}
\newcommand*\abs[1]{\left \vert #1 \right\vert}
\DeclareMathOperator{\Tr}{Tr}
\DeclarePairedDelimiterX{\inp}[2]{\langle}{\rangle}{#1, #2}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem*{corollary}{Corollary}
\newtheorem*{lemma}{Lemma}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}



\author{%
\textsc{Pranshu Gaba} \thanks{:)} \\[1ex]
\normalsize Indian Institute of Science, Bangalore \\
\normalsize \href{mailto:gabapranshu@iisc.ac.in}{gabapranshu@ug.iisc.in}}
\title{Hermitian Forms and Zeros of a Polynomial}
\date{\today}

\renewcommand{\maketitlehookd}{%

\begin{abstract}
We looked at the general properties of Hermitian (self-adjoint) matrices, and used the Schur-Cohn theorem to find the number of roots of a polynomial lying within and without the unit circle. 
\end{abstract}
}


\begin{document}
\maketitle

\section{Introduction}

%\lettrine[nindent=0em,lines=2]{I}

In this paper we see the properties of Hermitian matrices, which are very useful and interesting. We also see and prove the Schur-Cohn theorem to find the number of roots of a polynomial lying within the unit circle. 

There are many ways to locate the roots of a polynomial. The Schur-Cohn theorem shows a surprising connection between linear algebra and roots of a polynomial. It will be used to find out how many roots of the polynomial lie inside and outside the unit circle.

First we will define some basic terms that will be used ahead in the paper.

\section{Definitions}



\subsection{Norm of a matrix}

\subsubsection{Operator norm}
Given \(A \in \mathbb{M}_n\), define \(\norm{A} =\displaystyle \sup _{x \neq 0} \frac{\norm{Ax}}{\norm{x}} = \sup_{\norm{x} = 1} \norm{Ax} \) to be the operator norm of \(A\). The triangle inequality \(\norm{A + B} \leq \norm{A} + \norm{B}\) is satisfied. 

\subsubsection{Hilbert-Schmidt norm}

The Hilbert-Schmidt norm of matrix \(A\), is defined as the square root of sum of squares of all entries in \(A\). 

 \[\norm{A}_2 = \left( \sum_{i, j} \abs{a_{ij}}^2 \right) ^{1/2}\]


The operator norm is always less than or equal to the Hilbert-Schmidt norm.

\subsection{Inner product}

The inner product is a binary operator on two vectors \(\inp{\cdot}{\cdot} \colon V  \times  V \to \mathbb{C}\).  satisfies the following conditions for all \(x, y, z \in V\) and \(a \in \mathbb{C}\):

\begin{itemize}
\item It is linear in the first term. 

\(\quad \inp{ax}{y} = a \inp{x}{y}\)
 
\(\quad \inp{x + y}{z} = \inp{x}{z} + \inp{y}{z}\)

\item It becomes its complex conjugate when the arguments are reversed.

\(\quad \inp{x}{y} = \conj{\inp{y}{x}}\)

\item Inner product of a vector with itself is non-negative. 

\(\quad \inp{x}{x} \ge 0\). Here equality is achieved if and only if \(x = 0\).
\end{itemize}



\subsection{Adjoint}

Let \(A \in \mathbb{M}_n (\mathbb{C})\),  a \(n \times n\) square matrix with complex entries. The adjoint of matrix \(A\), denoted by \(\adj{A}\), is the matrix that  satisfies \(\inp{\adj{A}x}{y} = \inp{x}{Ay}\). 

We see, from the properties of inner product, that the adjoint of a matrix is obtained by taking its transpose, followed by taking the complex conjugate of every element. 

If the \(ij^{\text{th}} \) of \(A\) is \(a_{ij}\), then the \(ij^{\text{th}}\) entry of \(\adj{A}\) is \(\conj{a_{ji}}\). It follows that \(\adj{(\adj{A})} = A\).

Note that \(\adj{A}\), like \(A\), represents a linear transformation on \(\mathbb{C}^n\).

\(a_{ij}\) is defined as \(\inp{Ae_j}{e_i}\).


\subsection{Positive Definite Matrix}

\begin{definition} A matrix \(A\in \mathbb{M}_n\) that satisfies \(\inp{Ax}{x} \ge 0\) for all \(x \in \mathbb{C}^n\) is called a {\em positive semidefinite matrix}. If the inequality is strict, \(\inp{Ax}{x} > 0\), then \(A\) is called a {\em positive definite matrix}.  \end{definition}

\begin{theorem}All eigenvalues of a  positive semidefinite matrix are non-negative. \end{theorem}
\begin{proof} Left to the reader. \end{proof}


\begin{lemma} If \(A \ge 0\), then \(\norm{A} = \sup _{\norm{x} = 1} \inp{Ax}{x}\) \end{lemma}
\begin{proof} Left to the reader. \end{proof}


\subsection{Unitary Matrices}

A square matrix \(U\) is a unitary matrix if \(\adj{U} U = I\). The determinant of a unitary matrix is 1. It preserves inner product, \(\inp{Ux}{Uy} = \inp{x}{y}\).

\begin{proof} Left to the reader \end{proof}

\subsection{Trace}
The trace of a matrix is the sum of the diagonal elements of the matrix. 

\(\Tr(A) = \sum_{i=1} ^n \inp{Ae_i}{ e_i}\)

\begin{theorem}The trace of \(\adj{A}A\) is equal to the Hilbert-Schmidt norm of \(A\). \(\Tr( \adj{A} A ) = \norm{A}_2\)\end{theorem}
\begin{proof} Left to the reader \end{proof}


\subsection{Hermitian Matrix}
A matrix that satisfies \(A = \adj{A}\) is called a Hermitian matrix (also known as self-adjoint matrix). 

\begin{theorem} \label{herm_eig_real} All eigenvalues of a Hermitian matrix are real. \end{theorem}

\begin{proof}
Let \(v\) be an eigenvector of a Hermitian matrix \(A\), and let \(\lambda\) be the corresponding eigenvalue. Then \(Av = \lambda v\). 

\(\inp{Av}{v} = \inp{\lambda v}{v} = \lambda\inp{v}{v}\). Also \(\inp{Av}{v} = \inp{v}{\adj{A}v} = \inp{v}{Av} = \inp{v}{\lambda v} = \conj{\lambda} \inp{v}{v}\)

This means \(\lambda \inp{v}{v} = \conj{\lambda} \inp{v}{v}\) for any \(v\). Since \(\lambda\) is the same conjugate as its complex conjugate, it implies \(\lambda\) is real. 
 \end{proof}


The converse of this is also true. 
\begin{theorem} [Converse of Theorem \ref{herm_eig_real}] If \(A \in \mathbb{M}_n (\mathbb{C})\) and \(\inp{Ax}{x} \in \mathbb{R}\) for every \(x\), then \(A = \adj{A}\). \end{theorem}

\begin{proof} Let \(\alpha \in \mathbb{C}\) and \(h, g \in \mathbb{C}^n\). Then
\(\inp{A(h + \alpha g)}{h + \alpha g} = \inp{Ah}{h} + \alpha \inp{Ag}{h} + \conj{\alpha} \inp{Ah}{g} + \abs{\alpha}^2 \inp{Ag}{g} \)

So \(\alpha \inp{Ag}{h} + \conj{\alpha} \inp{Ah}{g} = \conj{\alpha} \inp{h}{Ag} + \alpha \inp{g}{Ah} \)

When \(\alpha = 1\), \(\inp{Ag}{h} + \inp{Ah}{g} = \inp{h}{Ag} + \inp{g}{Ah}\)\

When \(\alpha = i\), \(i \inp{Ag}{h} - i \inp{Ah}{g} = -i \inp{h}{Ag} + i \inp{g}{Ah}\)

\(2i \inp{Ag}{h} = 2i \inp{g}{Ah}\) ot \(\inp{Ag}{h} = \inp{g}{Ah} = \inp{\adj{A}g}{h}\)

\(Ag = \adj{A}g\) for all \(g\), therefore \(A = \adj{A}\). \(A\) is Hermitian.
 \end{proof}

\begin{corollary}Every positive semidefinite matrix is Hermitian.\end{corollary}
\begin{proof} Left to the reader. \end{proof}

The converse is also true.
\begin{theorem} \(\adj{A} A\) is always positive semidefinite. \end{theorem}

\begin{proof} \(\inp{\adj{A} A x}{ x} = \inp{Ax}{Ax} = \norm{Ax}^2 \ge 0\)\end{proof}


\begin{lemma} If \(A\) is Hermitian, then \(\norm{A} = \sup_{\norm{h} = 1} \abs{\inp{Ah}{h}}\).\end{lemma}
\begin{proof} Left to the reader \end{proof}


\begin{lemma} If \(\inp{Ah}{h} = 0\) for all \(h\), then \(A = 0\).\end{lemma}
\begin{proof} Left to the reader \end{proof}


\subsection{Diagonalization}

Hermitian matrices can be diagonalized. For every Hermitian matrix \(A\), there exists a diagonal matrix \(\Lambda\) such that \(A = \adj{U}  \Lambda U\). Here \(U\) is some unitary matrix. 


\subsection{Projectors}
A matrix \(P\) is a projector if \(P^2 = P\) and \(\adj{P} = P\)

\begin{theorem} There exists a subspace \(M\) of \(\mathbb{C}^n\) such that \(Pm = m \ \forall m \in M\), and \(P x = 0 \ \forall x \in m^\perp\) \end{theorem}
\begin{proof} Left to the reader \end{proof}

\subsection{Shift Matrix}
Let \(S\), the {\em shift matrix},  be the \(n \times n\) square matrix \( \begin{bmatrix} 
0 & 1 & 0 & \ldots & 0 \\
0 & 0 & 1 & \ldots & 0 \\
\vdots & \vdots & \vdots &\ddots & \vdots \\
0 & 0 & 0 &\ldots & 1 \\
0 & 0 & 0 & \ldots & 0 \\ 
\end{bmatrix}\). 

 Note that \(S\) is a nilpotent matrix of order \(n\), i.e. \(S^n\) is a zero matrix.

Related useful matrices: \(\adj{S}\), \(S \adj{S}\), \(I - S \adj{S}\). The last one is a projector. 


\section{Schur-Cohn Theorem}

Given any polynomial \(p(z) = a_0 z^n + a_1z^{n-1} + \cdots + a_n\) with complex coefficients, we are interested in finding how many of its roots lie within the unit circle and how many roots lie outside. Without loss of generality, let \(a_0 = 1\) as it does not change the roots of the polynomial. 

Suppose \(p\) has roots \(\alpha_i\). Then \(p(z) = (z - \alpha_1) (z - \alpha_2) \cdots (z - \alpha_n)\). 

 Then \(p(S) \) is \( \begin{bmatrix} 

% replace zeroes with dots
a_n & a_{n-1} & \ddots & \ddots & a_1 \\
0 & a_n & a_{n-1} & \ddots & \ddots \\
0 & 0 & a_n & \ddots & \ddots \\
0 & 0 & 0 &\ddots & a_{n-1} \\
0 & 0 & 0 & 0 & a_n \\ 
\end{bmatrix}\). 


This can be factorized as \(p(S) = (S - \alpha_1I) (S - \alpha_2 I) \cdots (S - \alpha_n I)\). Let \(B_j = S - \alpha_jI\).

Next, define \(q\) as the polynomial \(\conj{a_n}z^n + \conj{a_{n-1}}z^{n-1} + \cdots + \conj{a_0}\). Note that its roots are \(\frac {1}{\conj{\alpha_i}}\). We get \(q(z) = (1 - \conj{\alpha_1}z) (1 - \conj{\alpha_2}z) \cdots (1 - \conj{\alpha_nz})\). Also, \(q(S) = (I - \conj{\alpha_1}S) (I - \conj{\alpha_2}S) \cdots (I - \conj{\alpha_n}S)\). Let \(C_j= I -  \conj{\alpha_j} S\).
%Explain how roots of q are obtained.


Let \(H\) be equal to \(\norm{ q(S) x }^2 - \norm{ p(S) x}^2\)

%derive this expression of H
\(H\) can also be written as \(\inp{(\adj{q(S)} q(S) - \adj{p(S)} p(S))x}{ x}\).

We can now state the Schur-Cohn theorem:

\begin{theorem}The polynomial \(p\), it will have \(k\) roots inside the circle, and \(n-k\) roots outside the circle iff \(k\) eigenvalues of \(H\) are positive and \(n-k\) are negative. \end{theorem}

\section{Proof}
%The proof is trivial and is left as an exercise to the reader. 

We will first prove the Schur-Cohn theorem for \(n =1\), that is for linear polynomials. It will then be extended to polynomials of higher degrees with the help of the Spectral theorem and the Courant-Fischer theorem. 


\subsection{Linear Polynomial}


Let's write \(q(S)\) and \(p(S)\) as a product of the linear terms. \(\adj{q(S)} q(S) - \adj{p(S)} p(S) \\= \adj{(C_1C_2C_3 \ldots C_n)}(C_1C_2C_3\ldots C_n) - \adj{(B_1B_2B_3\ldots B_n)} (B_1B_2B_3\ldots B_n)\)

Let's look at \(\adj{C_1} C_1 - \adj{B_1} B_1\) first. Substituting the values of \(C_1\) and \(B_1\), we get 

\begin{equation*}
\begin{split}
& \phantom{=}    \adj{C_1}C_1 - \adj{B_1} B_1 \\
 & = \adj{(I - \conj{\alpha_1}S)} (I - \conj{\alpha_1}S) - \adj{(S - \alpha_1 I)} (S - \alpha_1 I) \\
& = (I - \alpha_1\adj{S}) (I - \conj{\alpha_1}S) - (\adj{S} - \conj{\alpha_1} I) (S - \alpha_1 I) \\
 & = (I - \alpha_1\adj{S} - \conj{\alpha_1}S + \abs{\alpha_1}^2 \adj{S} S) - (\adj{S} S - \alpha_1 \adj{S} - \conj{\alpha_1} S + \abs{\alpha_1}^2I)\\
& = I - \abs{\alpha_1}^2 I - \adj{S} S + \abs{\alpha_1}^2 \adj{S} S \\
& = (1 - \abs{\alpha_1}^2) (I - \adj{S} S)
\end{split}
\end{equation*}

Note that \(I - \adj{S} S\) is a posititve definite matrix. If \(\abs{\alpha} < 1\), then the root of the linear polynomial lies within the unit circle. Also note that \(H\) has one negative eigenvalue. Similarly, if \(\abs{\alpha} > 1\), then the root of the linear polynomial lies outside the unit circle, and the eigenvalue of \(H\) is positive. This shows that the Schur-Cohn theorem is true for \(n = 1\). We will now extend the proof for all \(n\). 

\subsection{Spectral theorem}

\begin{theorem} Let \(A \in \mathbb{M}_n\) be a Hermitian matrix with eigenvalues \(\lambda_1 \le \lambda_2 \le \lambda_3 \le \cdots \le \lambda_n\). Then \(A\) can be written as \(U \Lambda \adj{U} \), where \(U\) is a unitary matrix, and \(\Lambda\) is a diagonal matrix with real entries. \end{theorem}

\begin{proof} Left to the reader. \end{proof}

\subsection{Courant-Fischer theorem}

\begin{theorem} Let \(A \in \mathbb{M}_n\) be a Hermitian matrix with eigenvalues \(\lambda_1 \le \lambda_2 \le \lambda_3 \le \cdots \le \lambda_n\). Then 

  \[ \lambda_k = \min_{\omega_1, \ldots , \omega_{n-k} \in \mathbb{C}^n} \max_{\substack{x \neq 0, x\in \mathbb{C}^n \\ x \perp \omega_1, \ldots , \omega_{n-k}}} \frac{\inp{Ax}{x}}{\inp{x}{x}}\]

 \end{theorem}




\begin{proof} If \(x \neq 0\), then \(\frac{\inp{Ax}{x}}{\inp{x}{x}} = \frac{\inp{U \Lambda \adj{U} x}{x}}{\inp{\adj{U} x}{ \adj{U} x}} \\
 = \frac{\inp{\Lambda \adj{U} x}{ \adj{U} x}}{\inp{\adj{U}x}{\adj{U}x}}\). and \( \{ \adj{U} x : x \neq 0\}  = \{ x \in \mathbb{C}^n : x \neq 0 \}\) 



Thus if \(\omega_1, \ldots , \omega_{n-k}\) are given, then 

\[ \sup_{\substack{x \neq 0 \\ x \perp \omega_1, \ldots, \ \omega_{n-k}}} \frac{\inp{Ax}{x}}{\inp{x}{x}} = \sup_{\substack{y \neq 0 \\ y \perp \adj{U}\omega_1, \ldots ,\ \adj{U} \omega_{n-k}}} \frac{\inp{\Lambda y}{y}}{\inp{y}{y}}\]

\(x \perp \omega\) if and only if \(y \perp \adj{U} \omega\). 

\[ = \sup_{\substack{\inp{y}{y} = 1 \\ y \perp \adj{U} \omega_1, \ \ldots , \ \adj{U}\omega_{n-k}}} \sum_{i = 1}^n \lambda_i \abs{y_i}\]

\[ \ge \sup_{\substack{\inp{y}{y} = 1 \\ y \perp \adj{U} \omega_1, \ \ldots , \ \adj{U}\omega_{n-k} \\ y_1 = y_2 = \cdots = y_k-1 = 0 }} \sum_{i = 1}^n \lambda_i \abs{x_i}^2\]

\[ = \sup_{\substack{\inp{y}{y} = 1 \\ y \perp \adj{U} \omega_1, \ \ldots , \ \adj{U}\omega_{n-k} \\ y_1 = y_2 = \cdots = y_k-1 = 0 }} \sum_{i = k}^n \lambda_i \abs{y_i}^2\]

\[ \ge \lambda_k \]


Let \(\omega_1 = x_n , \ldots , \ \omega_{n-k} = x_{k + 1}\)

If \(x\perp \omega_i\), as above, then \(x = \sum_{i = 1} ^k c_i x_i\). 

\(\inp{Ax}{x} = \inp{A \sum_{i = 1} ^ n c_i X_i}{ \sum_{i = 1} ^n c_i x_i}\)

\( = \inp{\sum_{i = 1} ^n c_i \lambda_i x_i}{\sum_{i = 1} ^n c_i x_i}\)

\(= \sum_{i = 1} ^ k \lambda_i \abs{c_i}^2\) 

\( \le \lambda_k \sum_{i = 1} ^{k} \abs{c_i}^2\)


\end{proof}

\section{Extensions}


\subsection{Arbitrary radius}

\subsection{Limitations}

\section{Conclusion}
Thank You!

\section{Acknowledgements}

\section{References}
\end{document}