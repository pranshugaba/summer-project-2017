

\documentclass[twofold]{article}

%\usepackage[sc]{mathpazo}
\usepackage{lmodern}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\linespread{1.05}
\usepackage{microtype}


\usepackage[english]{babel}

\usepackage[hmarginratio=1:1, top=32mm,columnsep=20pt]{geometry}
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption}
\usepackage{booktabs}

\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{abstract}
\renewcommand{\abstractnamefont}{\normalfont\bfseries}
\renewcommand{\abstracttextfont}{\normalfont\small}

\usepackage{lettrine}

%\usepackage{titlesec}
%\renewcommand\thesection{\Roman{section}}
%\renewcommand\thesubsection{\roman{subsection}}
%\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{}
%\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[C]{Pranshu Gaba $\vert$ Summer Project 2017 $\vert$ Sr. No. 13718}
\fancyfoot[RO, LE]{\thepage}


\usepackage{titling}
\usepackage{hyperref}

\usepackage{mathtools}

%\setlength{\droptitle}{-6\baselineskip}
\pretitle{\begin{center}\huge\bfseries}
\posttitle{\end{center}}


\newcommand*\conj[1]{\overline{#1}}
\newcommand*\adj[1]{#1^*}
\newcommand*\norm[1]{\left \Vert #1 \right\Vert}
\newcommand*\abs[1]{\left \vert #1 \right\vert}
\newcommand*\trp[1]{#1^T}
\DeclareMathOperator{\Tr}{Tr}
\DeclarePairedDelimiterX{\inp}[2]{\langle}{\rangle}{#1, #2}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem*{corollary}{Corollary}
\newtheorem*{lemma}{Lemma}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\author{%
\textsc{Pranshu Gaba}  \\[1ex]
\normalsize Indian Institute of Science, Bangalore \\
\normalsize \href{mailto:gabapranshu@iisc.ac.in}{gabapranshu@ug.iisc.in}}
\title{Hermitian Forms and Zeros of a Polynomial}
\date{\today}

\renewcommand{\maketitlehookd}{%

\begin{abstract}
We looked at the general properties of Hermitian (self-adjoint) matrices, and used the Schur-Cohn theorem to find the number of roots of a polynomial lying within and without the unit circle. 
\end{abstract}
}


\begin{document}
\maketitle

\section{Introduction}

%\lettrine[nindent=0em,lines=2]{I}

In this paper we see the properties of Hermitian matrices, which are very useful and interesting. We also see and prove the Schur-Cohn theorem to find the number of roots of a polynomial lying within the unit circle. 

There are many ways to locate the roots of a polynomial. The Schur-Cohn theorem shows a surprising connection between linear algebra and roots of a polynomial. It will be used to find out how many roots of the polynomial lie inside and outside the unit circle.

First we will define some basic terms that will be used ahead in the paper.

\section{Definitions}



\subsection{Norm of a matrix}

\subsubsection{Operator norm}
Given \(A \in \mathbb{M}_n\) (the set of \(n \times n\) square matrices with complex elements), the operator norm of \(A\), denoted by \(\norm{A}\), is defined as \[\norm{A} =\displaystyle \sup _{x \neq 0} \frac{\norm{Ax}}{\norm{x}} = \sup_{\norm{x} = 1} \norm{Ax} \]

 The triangle inequality \(\norm{A + B} \leq \norm{A} + \norm{B}\) is satisfied. 

\subsubsection{Hilbert-Schmidt norm}

The Hilbert-Schmidt norm of matrix \(A\), denoted by \(\norm{A}_2\),  is defined as the square root of sum of squares of all entries in \(A\). 

 \[\norm{A}_2 = \left( \sum_{i, j} \abs{a_{ij}}^2 \right) ^{1/2}\]


\begin{theorem} The operator norm is always less than or equal to the Hilbert-Schmidt norm. \end{theorem}

\begin{proof}
 \begin{equation*} \begin{split} 
\norm{Ax}^2 & = \sum_{i = 1} ^ n \abs{\sum_{j = 1} ^ n a_{ij} x_j}^2 \\
& \le  \sum_{i = 1} ^ n \left( \sum_{j = 1} ^ n \abs{a_{ij}} \abs{ x_j}\right) ^2 \\
& \le \left( \sum_{i = 1}^n \sum_{j = 1}^n \abs{a_{ij}}^2\right) \left( \sum_{j=1}^n \abs{x_j}^2 \right)  = \left( \sum_{i, j} \abs{a_{ij}}^2 \right) \norm{x}^2
\end{split}
\end{equation*} 

Therefore \(\displaystyle \frac{\norm{Ax}}{\norm{x}} \le \left( \sum_{i, j} \abs{a_{ij}}^2 \right) ^{1/2}\), which is equivalent to \(\norm{A} \le \norm{A}_2\).
\end{proof}



\subsection{Inner product}

The inner product is a binary operator on two vectors \(\inp{\cdot}{\cdot} \colon V  \times  V \to \mathbb{C}\). It  satisfies the following conditions for all \(x, y, z \in V\) and \(a \in \mathbb{C}\):

\begin{itemize}
\item It is linear in the first term. 

\(\quad \inp{ax}{y} = a \inp{x}{y}\)
 
\(\quad \inp{x + y}{z} = \inp{x}{z} + \inp{y}{z}\)

\item It becomes its complex conjugate when the arguments are reversed.

\(\quad \inp{x}{y} = \conj{\inp{y}{x}}\)

\item Inner product of a vector with itself is non-negative. 

\(\quad \inp{x}{x} \ge 0\). Here equality is achieved if and only if \(x = 0\).
\end{itemize}

For vectors on \(\mathbb{C}^n\), the inner product \(\inp{x}{y}\) is defined as \(\trp{x} \conj{y}\), the vector multiplication of the transpose of the first term with the complex conjugate of the second term. This definition satisfies all the above mentioned conditions.


\subsection{Adjoint}

Let \(A \in \mathbb{M}_n\). The adjoint of matrix \(A\), denoted by \(\adj{A}\), is the matrix that  satisfies \(\inp{\adj{A}x}{y} = \inp{x}{Ay}\). 

\begin{theorem} The adjoint of a matrix is obtained by taking the complex conjugate of every element, followed by transposing the matrix. \end{theorem}


\begin{proof} Using properties of inner product on \(\mathbb{C}^n\),

\begin{equation*} \begin{split}
\inp{\adj{A}x}{y} & = \trp{(\adj{A}x)} \conj{y}  \\
& = \trp{x} \trp{(\adj{A})} \conj{y}\\
= \inp{x}{Ay} & = \trp{x} \conj{Ay} 
\end{split} \end{equation*}

Therefore \(\trp{(\adj{A})} = \conj{A}\), or \(\adj{A} = \trp{(\conj{A})}\). \end{proof}

Note that the adjoint of the adjoint of a matrix is the original matrix itself, \(\adj{(\adj{A})} = A\).

The adjoint of a matrix, \(\adj{A}\), like \(A\), represents a linear transformation on \(\mathbb{C}^n\).


\subsection{Positive Definite Matrix}

A matrix \(A\in \mathbb{M}_n\) that satisfies \(\inp{Ax}{x} \ge 0\) for all \(x \in \mathbb{C}^n\) is called a {\em positive semidefinite matrix} and is denoted as \(A \ge 0\). If the inequality is strict, \(\inp{Ax}{x} > 0\), then \(A\) is called a {\em positive definite matrix}, and it is denoted as \(A > 0\). 

\begin{theorem} Let \(A \in \mathbb{M}_n\) be a positive semidefinite matrix. Then all eigenvalues of \(A\) are non-negative. \end{theorem}
\begin{proof} Let \(\lambda_1, \lambda_2, \ldots , \lambda n\) be the eigenvalues of \(A\), and let \(x_1, x_2, \ldots , x_n\) be the corresponding eigenvectors. For any eigenvector \(x_i\), \(\inp{Ax_i}{x_i} = \inp{\lambda_i x_i}{x_i} = \lambda_i \inp{x_i}{x_i}\). 

\(\lambda_i = \frac{\inp{Ax_i}{x_i}}{\inp{x_i}{x_i}}\)

\(\inp{Ax_i}{x_i} \) is non-negative because \(A\) is positive semidefinite, and \(\inp{x_i}{x_i}\) is positive by definition of inner product. Hence \(\lambda_i \ge 0\). All eigenvalues of \(A\) are non-negative. \end{proof}


\begin{lemma} If \(A \ge 0\), then \(\displaystyle \norm{A} = \sup _{\norm{x} = 1} \inp{Ax}{x}\) \end{lemma}
%check this proof. Looks wrong
\begin{proof} \(\inp{Ax}{x} \le \norm{Ax} \norm{x} \le \norm{A} \norm{x}^2\). So \(\sup_{\norm{x} = 1} \inp{Ax}{x} \leq \norm{A} \) \end{proof}

\begin{theorem} \(\adj{A} A\) is a positive semidefinite for all \(A \in \mathbb{M}_n\). \end{theorem}
\begin{proof} \(\inp{\adj{A} A x}{ x} = \inp{Ax}{Ax} = \norm{Ax}^2 \ge 0\)\end{proof}

\subsection{Unitary Matrices}

A square matrix \(U\) that satisfies \(\adj{U} U = I\) is called a unitary matrix.

\begin{theorem} The columns of a unitary matrix \(U\) form an orthonormal basis. \end{theorem}
\begin{proof} \(U^{-1}\) exists, so each column is linearly independent.   \end{proof}


\begin{theorem} It preserves inner product, \(\inp{Ux}{Uy} = \inp{x}{y}\). \end{theorem}

\begin{proof} \(\inp{x}{y} = \inp{Ix}{y} = \inp{\adj{U}U x}{y} = \inp{Ux}{Uy}\) \end{proof}

\subsection{Trace}
The trace of matrix \(A\) is the sum of the diagonal elements of the matrix.

\[\Tr(A) = \sum_{i=1} ^n a_{ii} = \sum_{i=1} ^n \inp{Ae_i}{ e_i}\]

\begin{remark}Here \(a_{ij}\) denotes the element in the \(i^{\text{th}}\) row and \(j^{\text{th}}\) column. \(e_i\) denotes the \(i^{\text{th}}\) standard basis vector. Note that \(a_{ij} = \inp{e_i}{e_j}\).\end{remark}

\begin{theorem}The trace of \(\adj{A}A\) is equal to the square of the Hilbert-Schmidt norm of \(A\). \(\Tr( \adj{A} A ) = \norm{A}_2^2\)\end{theorem}
\begin{proof} \begin{equation*} \begin{split} 
\Tr(\adj{A} A) & = \sum_{i=1}^n \inp{\adj{A}A e_i}{e_i} \\
& = \sum_{i=1} ^n \inp{Ae_i}{Ae_i} \\
& = \sum_{i=1} ^n \norm{Ae_i}^2 \\
& = \sum_{i=1} ^n \sum_{j=1} ^n \abs{a_{ji}}^2 \\
& = \norm{A}^2_2
\end{split} \end{equation*}
 \end{proof}

\subsection{Hermitian Matrix}
A matrix that satisfies \(A = \adj{A}\) is called a Hermitian matrix (also known as self-adjoint matrix). 

\begin{theorem} \label{herm_eig_real} All eigenvalues of a Hermitian matrix are real. \end{theorem}

\begin{proof}
%format equations
Let \(v\) be an eigenvector of a Hermitian matrix \(A\), and let \(\lambda\) be the corresponding eigenvalue. Then \(Av = \lambda v\). 

\(\inp{Av}{v} = \inp{\lambda v}{v} = \lambda\inp{v}{v}\). Also \(\inp{Av}{v} = \inp{v}{\adj{A}v} = \inp{v}{Av} = \inp{v}{\lambda v} = \conj{\lambda} \inp{v}{v}\)

This means \(\lambda \inp{v}{v} = \conj{\lambda} \inp{v}{v}\) for any \(v\). Since \(\lambda\) is the same conjugate as its complex conjugate, it implies \(\lambda\) is real. 
 \end{proof}


The converse of this is also true. 
\begin{theorem}\label{eig_real_herm}  [Converse of Theorem \ref{herm_eig_real}]  If \(A \in \mathbb{M}_n\) and \(\inp{Ax}{x} \in \mathbb{R}\) for every \(x\), then \(A = \adj{A}\). \end{theorem}

\begin{proof} Let \(\alpha \in \mathbb{C}\) and \(h, g \in \mathbb{C}^n\). Then
\(\inp{A(h + \alpha g)}{h + \alpha g} = \inp{Ah}{h} + \alpha \inp{Ag}{h} + \conj{\alpha} \inp{Ah}{g} + \abs{\alpha}^2 \inp{Ag}{g} \)

So \(\alpha \inp{Ag}{h} + \conj{\alpha} \inp{Ah}{g} = \conj{\alpha} \inp{h}{Ag} + \alpha \inp{g}{Ah} \)
When \(\alpha = 1\), \(\inp{Ag}{h} + \inp{Ah}{g} = \inp{h}{Ag} + \inp{g}{Ah}\)\

When \(\alpha = i\), \(i \inp{Ag}{h} - i \inp{Ah}{g} = -i \inp{h}{Ag} + i \inp{g}{Ah}\)

\(2i \inp{Ag}{h} = 2i \inp{g}{Ah}\) or \(\inp{Ag}{h} = \inp{g}{Ah} = \inp{\adj{A}g}{h}\)

\(Ag = \adj{A}g\) for all \(g\), therefore \(A = \adj{A}\). \(A\) is Hermitian.
 \end{proof}

\begin{corollary}Every positive semidefinite matrix \(A \in \mathbb{M}_n\) is Hermitian.\end{corollary}
\begin{proof} \(\inp{Ax}{x} \ge 0 \)  so \(\inp{Ax}{x} \in \mathbb{R}\) for all \(x\). By Theorem \ref{eig_real_herm}, \(A\) is Hermitian. \end{proof}




\begin{lemma} If \(A\) is Hermitian, then \(\norm{A} = \sup_{\norm{h} = 1} \abs{\inp{Ah}{h}}\).\end{lemma}
\begin{proof} Let \(x \in \mathbb{C}^n\). Then for any \(g \in \mathbb{C}^n\), we have \(\abs{\inp{x}{g}} \le \norm{x} \norm{g}\). (By Cauchy-Schwartz).

So \(\abs{\inp{x}{\frac{g}{\norm{g}}}} \le \norm{x}\) 

So \(\sup_{\norm{g} = 1} \abs{\inp{x}{g}} \le \norm{x} \) (Equality when \(g = \frac{x}{\norm{x}})\).

\(\norm{A} = \sup_{\norm{h} = 1} \norm{Ah}\)

\(= \sup_{\norm{h} = 1} \sup_{\norm{g}=1} \abs{\inp{Ah}{g}}  \)

If \(h, g \in \mathbb{C}^n\) with \(\norm{h} = \norm{g}\), then

\begin{equation*} \begin{split}
\inp{A (h \pm g)}{h\pm g} & = \inp{Ah}{h} \pm \inp{Ah}{g} \pm \inp{Ag}{h} + \inp{Ag}{g} \\
& = \inp{Ah}{h} \pm \inp{Ah}{g} \pm \inp{g}{Ah} + \inp{Ag}{g} \\
& = \inp{Ah}{h} \pm 2 \text{Re}(\inp{Ah}{g}) + \inp{Ag}{g} \\ \\
4 \text{Re}\inp{Ah}{g} = 
%complete this proof
\end{split} \end{equation*}
\end{proof}


\begin{lemma} If \(\inp{Ah}{h} = 0\) for all \(h\), then \(A = 0\).\end{lemma}
\begin{proof} Left to the reader \end{proof}


\subsection{Diagonalization}

Hermitian matrices can be diagonalized?

\begin{theorem}  For every Hermitian matrix \(A\), there exists a diagonal matrix \(\Lambda\) such that \(A = \adj{U}  \Lambda U\). Here \(U\) is some unitary matrix. \end{theorem}

\begin{proof} 
The matrix \(U\) is a unitary matrix, which means \(\adj{U} U = I\)



Start with any eigenvalue \(\lambda_i\) of \(A\). Let \(x_i\) be a unit eigenvector corresponding to \(\lambda_i\).

Let \(\mathcal{M} = \{y \in \mathbb{C}^n, \ \inp{y}{x} = 0\}\)

\(\inp{Ay}{x} = \inp{y}{Ax} = \lambda \inp{y}{x} = 0\).

% finish the proof
\end{proof}
\subsection{Projectors}
A matrix \(P\) is a projector if \(P^2 = P\) and \(\adj{P} = P\)

\begin{theorem} There exists a subspace \(M\) of \(\mathbb{C}^n\) such that \(Pm = m \ \forall m \in M\), and \(P x = 0 \ \forall x \in m^\perp\) \end{theorem}
\begin{proof} Left to the reader \end{proof}

\subsection{Shift Matrix}
Let \(S\), the {\em shift matrix},  be the \(n \times n\) square matrix \( \begin{bmatrix} 
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots &\ddots & \vdots \\
0 & 0 & 0 &\cdots & 1 \\
0 & 0 & 0 & \cdots & 0 \\ 
\end{bmatrix}\). 

 Note that \(S\) is a nilpotent matrix of order \(n\), i.e. \(S^n\) is a zero matrix.

Related useful matrices: 

\(\adj{S} =  \begin{bmatrix} 
0 & 0 & \cdots & 0 & 0 \\
1 & 0 & \cdots & 0 & 0 \\
0 & 1 & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots &\vdots & \vdots \\
0 & 0 & \cdots & 1 & 0 \\ 
\end{bmatrix}\). 

 \(\adj{S} S  =  \begin{bmatrix} 
0 & 0 & 0 & \cdots & 0 & 0 \\
0 & 1 & 0 & \cdots & 0 & 0 \\
0 & 0 & 1 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots &\vdots & \vdots \\
0 & 0 & 0 & \cdots & 1 & 0 \\ 
0 & 0 & 0 & \cdots & 0 & 1
\end{bmatrix}\). 

 \(I - S \adj{S} =  \begin{bmatrix} 
1 & 0 & \cdots & 0 & 0 \\
0 & 0 & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots &\vdots & \vdots \\
0 & 0 & \cdots & 0 & 0 \\ 
0 & 0 & \cdots & 0 & 0
\end{bmatrix}\). 

The last one \(I - \adj{S} S\) is a projector. 


\section{Schur-Cohn Theorem}

Given any polynomial \(p(z) = a_0 z^n + a_1z^{n-1} + \cdots + a_n\) with complex coefficients, we are interested in finding how many of its roots lie within the unit circle and how many roots lie outside. Without loss of generality, let \(a_0 = 1\) as it does not change the roots of the polynomial. 

Suppose \(p\) has roots \(\alpha_i\). Then \(p(z) = (z - \alpha_1) (z - \alpha_2) \cdots (z - \alpha_n)\). 

 Then \(p(S) \) is \( \begin{bmatrix} 

% replace zeroes with dots
a_n & a_{n-1} & \ddots & \ddots & a_1 \\
0 & a_n & a_{n-1} & \ddots & \ddots \\
0 & 0 & a_n & \ddots & \ddots \\
0 & 0 & 0 &\ddots & a_{n-1} \\
0 & 0 & 0 & 0 & a_n \\ 
\end{bmatrix}\). 


This can be factorized as \(p(S) = (S - \alpha_1I) (S - \alpha_2 I) \cdots (S - \alpha_n I)\). Let \(B_j = S - \alpha_jI\).

Next, define \(q\) as the polynomial \(\conj{a_n}z^n + \conj{a_{n-1}}z^{n-1} + \cdots + \conj{a_0}\). Note that its roots are \(\frac {1}{\conj{\alpha_i}}\). We get \(q(z) = (1 - \conj{\alpha_1}z) (1 - \conj{\alpha_2}z) \cdots (1 - \conj{\alpha_nz})\). Also, \(q(S) = (I - \conj{\alpha_1}S) (I - \conj{\alpha_2}S) \cdots (I - \conj{\alpha_n}S)\). Let \(C_j= I -  \conj{\alpha_j} S\).
%Explain how roots of q are obtained.


Let \(\underline{H}\) be the Hermitian form \(\norm{ q(S) x }^2 - \norm{ p(S) x}^2\). 

\(= \inp{q(S)x}{q(S)x} - \inp{p(S)x}{p(S)x}\)

\(= \inp{\adj{q}(S) q(S) x}{x} - \inp{\adj{p}(S) p(S) x}{x}\)

\(= \inp{(\adj{q}(S) q(S) - \adj{p} (S) p(S) ) x}{x}\).

The \(n \times n\) matrix corresponding to this Hermitian form is  \(H = \adj{q}(S) q(S) - \adj{p}(S) p(S)\).


We can now state the Schur-Cohn theorem:

\begin{theorem}The polynomial \(p\), it will have \(k\) roots inside the circle, and \(n-k\) roots outside the circle iff \(k\) eigenvalues of \(H\) are positive and \(n-k\) are negative. \end{theorem}

\section{Proof}
%The proof is trivial and is left as an exercise to the reader. 

We will first prove the Schur-Cohn theorem for \(n =1\), that is for linear polynomials. It will then be extended to polynomials of higher degrees with the help of the Spectral theorem and the Courant-Fischer theorem. 


\subsection{Linear Polynomial}


Let's write \(q(S)\) and \(p(S)\) as a product of the linear terms. \(\adj{q(S)} q(S) - \adj{p(S)} p(S) \\= \adj{(C_1C_2C_3 \ldots C_n)}(C_1C_2C_3\ldots C_n) - \adj{(B_1B_2B_3\ldots B_n)} (B_1B_2B_3\ldots B_n)\)

For \(n =1\), this is equal to \(\adj{C_1} C_1 - \adj{B_1} B_1\). 

\begin{equation*}
\begin{split}
&    \adj{C_1}C_1 - \adj{B_1} B_1 \\
 & = \adj{(I - \conj{\alpha_1}S)} (I - \conj{\alpha_1}S) - \adj{(S - \alpha_1 I)} (S - \alpha_1 I) \\
& = (I - \alpha_1\adj{S}) (I - \conj{\alpha_1}S) - (\adj{S} - \conj{\alpha_1} I) (S - \alpha_1 I) \\
 & = (I - \alpha_1\adj{S} - \conj{\alpha_1}S + \abs{\alpha_1}^2 \adj{S} S) - (\adj{S} S - \alpha_1 \adj{S} - \conj{\alpha_1} S + \abs{\alpha_1}^2I)\\
& = I - \abs{\alpha_1}^2 I - \adj{S} S + \abs{\alpha_1}^2 \adj{S} S \\
& = (1 - \abs{\alpha_1}^2) (I - \adj{S} S) \\
\end{split}
\end{equation*}

For \(n = 1\), \(I - \adj{S}S\) is just a \(1 \times 1\) matrix, so \(H = (1 - \abs{\alpha_1}^2)\). 



Note that \(I - \adj{S} S\) is a posititve definite matrix. If \(\abs{\alpha} < 1\), then root of the linear polynomial lies within the unit circle. Also note that \(H\) has one negative eigenvalue. Similarly, if \(\abs{\alpha} > 1\), then the root of the linear polynomial lies outside the unit circle, and the eigenvalue of \(H\) is positive. This shows that the Schur-Cohn theorem is true for \(n = 1\). We will now extend the proof for all \(n\). 

\subsection{Spectral theorem}

\begin{theorem} Let \(A \in \mathbb{M}_n\) be a Hermitian matrix with eigenvalues \(\lambda_1 \le \lambda_2 \le \lambda_3 \le \cdots \le \lambda_n\). Then \(A\) can be written as \(U \Lambda \adj{U} \), where \(U\) is a unitary matrix, and \(\Lambda\) is a diagonal matrix with real entries. \end{theorem}

\begin{proof} Left to the reader. \end{proof}

\subsection{Courant-Fischer theorem}

\begin{theorem} Let \(A \in \mathbb{M}_n\) be a Hermitian matrix with eigenvalues \(\lambda_1 \le \lambda_2 \le \lambda_3 \le \cdots \le \lambda_n\). Then 

  \[ \lambda_k = \min_{\omega_1, \ldots , \omega_{n-k} \in \mathbb{C}^n} \max_{\substack{x \neq 0, x\in \mathbb{C}^n \\ x \perp \omega_1, \ldots , \omega_{n-k}}} \frac{\inp{Ax}{x}}{\inp{x}{x}}\]

 \end{theorem}




\begin{proof} If \(x \neq 0\), then \(\frac{\inp{Ax}{x}}{\inp{x}{x}} = \frac{\inp{U \Lambda \adj{U} x}{x}}{\inp{\adj{U} x}{ \adj{U} x}} \\
 = \frac{\inp{\Lambda \adj{U} x}{ \adj{U} x}}{\inp{\adj{U}x}{\adj{U}x}}\). and \( \{ \adj{U} x : x \neq 0\}  = \{ x \in \mathbb{C}^n : x \neq 0 \}\) 



Thus if \(\omega_1, \ldots , \omega_{n-k}\) are given, then 

\[ \sup_{\substack{x \neq 0 \\ x \perp \omega_1, \ldots, \ \omega_{n-k}}} \frac{\inp{Ax}{x}}{\inp{x}{x}} = \sup_{\substack{y \neq 0 \\ y \perp \adj{U}\omega_1, \ldots ,\ \adj{U} \omega_{n-k}}} \frac{\inp{\Lambda y}{y}}{\inp{y}{y}}\]

\(x \perp \omega\) if and only if \(y \perp \adj{U} \omega\). 

\[ = \sup_{\substack{\inp{y}{y} = 1 \\ y \perp \adj{U} \omega_1, \ \ldots , \ \adj{U}\omega_{n-k}}} \sum_{i = 1}^n \lambda_i \abs{y_i}\]

\[ \ge \sup_{\substack{\inp{y}{y} = 1 \\ y \perp \adj{U} \omega_1, \ \ldots , \ \adj{U}\omega_{n-k} \\ y_1 = y_2 = \cdots = y_k-1 = 0 }} \sum_{i = 1}^n \lambda_i \abs{x_i}^2\]

\[ = \sup_{\substack{\inp{y}{y} = 1 \\ y \perp \adj{U} \omega_1, \ \ldots , \ \adj{U}\omega_{n-k} \\ y_1 = y_2 = \cdots = y_k-1 = 0 }} \sum_{i = k}^n \lambda_i \abs{y_i}^2\]

\[ \ge \lambda_k \]


Let \(\omega_1 = x_n , \ldots , \ \omega_{n-k} = x_{k + 1}\)

If \(x\perp \omega_i\), as above, then \(x = \sum_{i = 1} ^k c_i x_i\). 

\(\inp{Ax}{x} = \inp{A \sum_{i = 1} ^ n c_i X_i}{ \sum_{i = 1} ^n c_i x_i}\)

\( = \inp{\sum_{i = 1} ^n c_i \lambda_i x_i}{\sum_{i = 1} ^n c_i x_i}\)

\(= \sum_{i = 1} ^ k \lambda_i \abs{c_i}^2\) 

\( \le \lambda_k \sum_{i = 1} ^{k} \abs{c_i}^2\)


\end{proof}


\subsection{Continuing the proof}

For general \(n\), 

\(\adj{q}(S) q(S) - \adj{p}(S) p(S)\) 

\(= \adj{(C_1C_2C_3 \ldots C_n)}(C_1C_2C_3\ldots C_n) - \adj{(B_1B_2B_3\ldots B_n)} (B_1B_2B_3\ldots B_n)\)

\(= \adj{C_n} \cdots \adj{C_1} C_n \cdots C_1 - \adj{B_n} \cdots \adj{B_1} B_n \cdots B_1\)

\(B_i\) commutes with \(C_j\). 

\(1 - \adj{S} S = e_1 \adj{e_1}\). 

\(\adj{C_j} C_j - \adj{B_j} B_j = (1 - \abs{\alpha_j}^2) (1 - \adj{S} S) = (1 - \abs{\alpha_j}^2) e_1 \adj{e_1}\)

We can add and subtract terms to get a telescoping series:

%Add telescoping series

We get \(H =\displaystyle \sum_{j=1} ^n (1-\abs{\alpha_j}^2) V_j \adj{V_j}\), where \(V_j = \adj{B_1} \cdots \adj{B_{j-1}} \adj{C_{j+1}} \cdots \adj{C_n} e_1\)
\section{Extensions}


\subsection{Arbitrary radius}
To find the number of roots of \(p(z)\) inside a circle of radius \(r\), repeat the above Schur Cohn theorem but with the polynomial \(p(\frac{z}{r})\) instead. 
\subsection{Limitations}
This method fails when the polynomial has one or more roots on the circle. 

\section{Conclusion}
Thank You!

\section{Acknowledgements}
Prof. Tirthankar Bhattacharya

\section{References}


\end{document}